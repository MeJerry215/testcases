{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch train basics\n",
    "\n",
    "torch 训练 需要数据集、网络结构定义、Loss函数、优化器。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0410], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0410], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0410], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0410], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0105], device='cuda:0', requires_grad=True)\n",
      "tensor(82.7912, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# linear regression\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "hidden_size = 64\n",
    "\n",
    "data = torch.rand(1024, hidden_size).float().cuda()\n",
    "weight = torch.rand(hidden_size, 1).float().cuda()\n",
    "bias = torch.rand(1,).float().cuda()\n",
    "expect = data @ weight + bias\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "model = LinearRegression().train().cuda()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx in range(0, 1024, batch_size):\n",
    "        x = data[idx: idx+batch_size]\n",
    "        y = expect[idx: idx + batch_size]\n",
    "        preds = model(x)\n",
    "        if idx == 0:\n",
    "            print(model.linear.bias)\n",
    "        loss = loss_fn(y, preds)\n",
    "        if idx == 0:\n",
    "            print(model.linear.bias)\n",
    "        optimizer.zero_grad()\n",
    "        if idx == 0:\n",
    "            print(model.linear.bias)\n",
    "        loss.backward()\n",
    "        if idx == 0:\n",
    "            print(model.linear.bias)\n",
    "        optimizer.step()\n",
    "        if idx == 0:\n",
    "            print(model.linear.bias)\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.9450, 14.9724, 15.9223, 17.0857, 15.0906, 14.8759, 14.2641, 16.3343,\n",
      "        15.6017, 17.0765, 12.9845, 15.5586, 16.9820, 15.9542, 17.7072, 16.3956,\n",
      "        17.0560, 15.6756, 15.6948, 16.3338, 17.4296, 15.3655, 17.1667, 15.1330,\n",
      "        15.4936, 15.1647, 16.7156, 13.6955, 14.7442, 16.0632, 12.8069, 15.1517,\n",
      "        16.1933, 14.7765, 14.2440, 14.2172, 14.4803, 15.4032, 14.9916, 14.3205,\n",
      "        12.7070, 13.8750, 13.9180, 16.8788, 16.4735, 16.1670, 13.4830, 14.4689,\n",
      "        13.8179, 15.6851, 14.7772, 14.4882, 16.2031, 14.0884, 13.5313, 17.5115,\n",
      "        17.2383, 16.0091, 16.2571, 17.0073, 15.2144, 14.9747, 13.7560, 15.4398],\n",
      "       device='cuda:0')\n",
      "tensor([15.9539, 14.9790, 15.9161, 17.0165, 15.1091, 14.8780, 14.2719, 16.3147,\n",
      "        15.5802, 17.0503, 13.0557, 15.5458, 16.9319, 15.9438, 17.6500, 16.3750,\n",
      "        17.0072, 15.6966, 15.6925, 16.3340, 17.3589, 15.3501, 17.0987, 15.1292,\n",
      "        15.4643, 15.1721, 16.6982, 13.7601, 14.7692, 16.0673, 12.8510, 15.1519,\n",
      "        16.1572, 14.7719, 14.2817, 14.2613, 14.4990, 15.4004, 14.9930, 14.3428,\n",
      "        12.7733, 13.9039, 13.9455, 16.8460, 16.4395, 16.1339, 13.5220, 14.4909,\n",
      "        13.8917, 15.6902, 14.7969, 14.5052, 16.1828, 14.1285, 13.5784, 17.4637,\n",
      "        17.1785, 16.0218, 16.2186, 17.0040, 15.2407, 14.9927, 13.8079, 15.4588],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# print(weight.view(-1))\n",
    "# print(model.linear.weight.view(-1))\n",
    "\n",
    "# print(bias)\n",
    "# print(model.linear.bias)\n",
    "\n",
    "print(expect[:batch_size].view(-1))\n",
    "print(model(data[:batch_size]).view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profiling torch module\n",
    "\n",
    "torch提供了了专门的profiler API 分析代码中耗时以及内存消耗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-17 08:19:56 40283:40283 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-01-17 08:20:00 40283:40283 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-01-17 08:20:00 40283:40283 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# profiler\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        with profiler.record_function(\"LINEAR PASS\"):\n",
    "            out = self.linear(input)\n",
    "\n",
    "        with profiler.record_function(\"MASK INDICES\"):\n",
    "            threshold = out.sum(axis=1).mean().item()\n",
    "            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)\n",
    "            hi_idx = torch.from_numpy(hi_idx).cuda()\n",
    "\n",
    "        return out, hi_idx\n",
    "\n",
    "model = MyModule(500, 10).cuda()\n",
    "input = torch.rand(128, 500).cuda()\n",
    "mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()\n",
    "\n",
    "# warm-up\n",
    "model(input, mask)\n",
    "\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "    out, idx = model(input, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                     MASK INDICES        69.08%        2.764s        99.87%        3.996s        3.996s           0 b    -953.67 Mb       2.79 Gb      -1.00 Kb             1  \n",
      "                                  cudaMemcpyAsync        30.76%        1.231s        30.76%        1.231s     410.328ms           0 b           0 b           0 b           0 b             3  \n",
      "                                      LINEAR PASS         0.07%       2.809ms         0.13%       5.201ms       5.201ms           0 b           0 b       5.00 Kb           0 b             1  \n",
      "                                      aten::addmm         0.06%       2.257ms         0.06%       2.316ms       2.316ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
      "                                        aten::sum         0.00%     166.000us         0.01%     224.000us     224.000us           0 b           0 b         512 b         512 b             1  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.001s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
